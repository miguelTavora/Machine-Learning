{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Aprendizagem automática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções para competição "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vector(Docs):\n",
    "\t\n",
    "\tdictionary =pickle.load(open('A45102_classificador.p','rb'))\n",
    "\n",
    "\tdocs = clear_data(Docs,False)\n",
    "\n",
    "\tdocs = stemming(Docs,3)\n",
    "\n",
    "\ttf_idf = dictionary['tf-idf']\n",
    "\n",
    "\treturn tf_idf.transform(docs)\n",
    "\n",
    "\n",
    "def binClassify(X):\n",
    "\n",
    "\tdictionary =pickle.load(open('A45102_classificador.p','rb'))\n",
    "\n",
    "\tlogReg = dictionary['bin']\n",
    "\n",
    "\treturn logReg.predict(X)\n",
    "\n",
    "\n",
    "def multiClassify(X):\n",
    "\n",
    "\tdictionary =pickle.load(open('A45102_classificador.p','rb'))\n",
    "\n",
    "\tlogReg = dictionary['multi']\n",
    "\n",
    "\treturn logReg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções contrução do ficheiro pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para criar um ficheiro pickle com o objeto tf-idf\n",
    "def builder_td_idf(Docs):\n",
    "\n",
    "\tdictionary = {}\n",
    "\n",
    "\tx_train = clear_data(Docs,False)\n",
    "\n",
    "\tx_train = stemming(Docs,3)\n",
    "\n",
    "\ttf_idf = TfidfVectorizer(min_df=3,token_pattern=r'\\b\\w+\\b',ngram_range=(1,2)).fit(Docs)\n",
    "\n",
    "\tdictionary['tf-idf'] = tf_idf\n",
    "\n",
    "\tpickle.dump(dictionary,open('A45102_classificador.p','wb'))\n",
    "\n",
    "\n",
    "#função para criar objeto classificador binário\n",
    "def builder_bin_classify(X,y):\n",
    "\n",
    "\tdictionary =pickle.load(open('A45102_classificador.p','rb'))\n",
    "\n",
    "\tprint(dictionary.keys())\n",
    "\n",
    "\tlogReg = LogisticRegression(C=2,multi_class='multinomial',max_iter=1000,penalty='l2',solver='saga').fit(X,y)\n",
    "\n",
    "\tdictionary['bin'] = logReg\n",
    "\n",
    "\tpickle.dump(dictionary,open('A45102_classificador.p','wb'))\n",
    "\n",
    "\n",
    "#função para criar objeto classificador multi-classe\n",
    "def builder_multi_classify(X,y):\n",
    "\n",
    "\tdictionary =pickle.load(open('A45102_classificador.p','rb'))\n",
    "\n",
    "\tprint(dictionary.keys())\n",
    "\n",
    "\tlogReg = LogisticRegression(C=1.9,multi_class='ovr',max_iter=1000,penalty='l1',solver='saga').fit(X,y)\n",
    "\n",
    "\tdictionary['multi'] = logReg\n",
    "\n",
    "\tpickle.dump(dictionary,open('A45102_classificador.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções conversão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de conversão dos indices para binário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para converter os indices de 1-4 para 0 e os indices 7-10 para 1 (multi-classe para binário)\n",
    "#@args \n",
    "#-> target pontuação das críticas\n",
    "#@return array em formato binário\n",
    "def convert_indexes_into_binary(target):\n",
    "\n",
    "\tsize = len(target)\n",
    "\n",
    "\tdata_converted = np.zeros(size).astype('int8')\n",
    "\n",
    "\tfor i in range(size):\n",
    "\t\tif target[i] > 4:\n",
    "\t\t\tdata_converted[i] = 1\n",
    "\n",
    "\treturn data_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de limpeza dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para remover caracteres que não constituem muita importância para a classificação\n",
    "#@args \n",
    "#-> docs são os dados\n",
    "#-> str_bytes é uma flag || true -> dados estão em bytes, false -> str unicode do python\n",
    "#@return dados limpos\n",
    "def clear_data(docs,str_bytes):\n",
    "\tif str_bytes:\n",
    "\t\t#converter codificação dos caracteres bytes ASCII para UTF-8\n",
    "\t\tdata = [doc.decode('UTF-8') for doc in docs]\n",
    "\n",
    "\t#tirar os caracteres mudança linha em HTML por espaço\n",
    "\tdocs = [doc.replace('<br />',' ') for doc in docs]\n",
    "\n",
    "\t#remover todos os caraceteres não alfabeticos e deixa os caracteres com acentos tipo é e ç\n",
    "\t#feito através de uma expressão regular\n",
    "\tdocs = [re.sub(r'[^a-zA-Z0-9\\u00C0-\\u00FF]+',' ',doc) for doc in docs]\n",
    "\treturn docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de stemming do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função que converte várias palavras relacionadas numa única\n",
    "#permite reduzir o tamanho da lista dos tokens\n",
    "#@args\n",
    "#-> docs são os dados\n",
    "#-> type_stemming (inteiro) tipo de stemming 1 - Porter Stemmer, 2 -> Lancaster Stemmer, outro -> Snowball Stemmer\n",
    "#@return dados com as palavras relacionadas convertidas convertidas numa unica palavra \n",
    "def stemming(docs,type_stemming):\n",
    "\tif type_stemming == 1:\n",
    "\t\t#criação do objeto Porter Stemmer\n",
    "\t\tstemmer_porter = PorterStemmer()\n",
    "\t\t#metodo join permite juntar strings depois fazer o stemming\n",
    "\t\t#tem de fazer isto a todas as palavras em cada documento por sua vez em todos os documentos\n",
    "\t\tstemming_porter = [' '.join([stemmer_porter.stem(word) for word in doc.split()]) for doc in docs]\n",
    "\t\treturn stemming_porter\n",
    "\t\n",
    "\telif type_stemming == 2:\n",
    "\t\tstemmer_lancaster = LancasterStemmer()\n",
    "\t\tstemming_lancaster = [' '.join([stemmer_lancaster.stem(word) for word in doc.split()]) for doc in docs]\n",
    "\t\treturn stemming_lancaster\n",
    "\n",
    "\telse:\n",
    "\t\tstemmer_snowball = SnowballStemmer(language='english')\n",
    "\t\tstemming_snowball = [' '.join([stemmer_snowball.stem(word) for word in doc.split()]) for doc in docs]\n",
    "\t\treturn stemming_snowball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função converção dos textos na representação tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para converte textos de strings em tokens \n",
    "#@args \n",
    "#-> docs são os dados\n",
    "#-> min_df_value (inteiro) número de vezes que a palavra deve aparecer em x corpus\n",
    "#-> num_characters (inteiro) tamanho minimo da palavra\n",
    "#-> n_grams (inteiro) para criar bi-grama, tri-gramas e quadri-gramas\n",
    "#@return objeto do tipo tf-idf\n",
    "def convert_str_to_tf_idf(docs, min_df_value,num_characters,n_grams):\n",
    "\t#converte o valor inteiro na expressão do token_pattern\n",
    "\tpattern = r'\\b'\n",
    "\tfor i in range(num_characters):\n",
    "\t\tpattern = pattern + '\\w'\n",
    "\tpattern = pattern + '+'+r'\\b'\n",
    "\t\n",
    "\n",
    "\tif n_grams == 1:\n",
    "\t\t#também pode ser feito CountVectorizer() e TfidfTransformer()\n",
    "\t\t#min_df -> escolhe só palavras que aparecam em pelo menos x (o valor do argumento) corpus  \n",
    "\t\t#token_pattern -> caracteres que tenham quatro ou mais caracteres\n",
    "\t\treturn TfidfVectorizer(min_df=min_df_value,token_pattern=pattern).fit(docs)\n",
    "\n",
    "\telif n_grams == 2 or n_grams == 3 or n_grams == 4:\n",
    "\n",
    "\t\treturn TfidfVectorizer(min_df=min_df_value,token_pattern=pattern,ngram_range=(1,n_grams)).fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para converter os dados numa matriz esparsa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para converte textos de strings em tokens \n",
    "#@args \n",
    "#-> tf_idf objeto do tipo td-idf\n",
    "#-> docs dados\n",
    "#@return matriz esparsa to tf-idf\n",
    "def convert_to_sparce_matrix(tf_idf, docs):\n",
    "\n",
    "\treturn tf_idf.transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código para testar os classificadores para novos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pickle.load(open(None,'rb'))#alterar o valor None para o nome do ficheiro\n",
    "\n",
    "\n",
    "#fazer load dos dados\n",
    "Docs = dictionary[None] #ir buscar a key para os dados\n",
    "\n",
    "\n",
    "#ir buscar a classificação correspondente\n",
    "y = dictionary[None] #ir buscar a key para a classificação\n",
    "\n",
    "\n",
    "#variável com os indices 0-1\n",
    "y0 = convert_indexes_into_binary(y)\n",
    "\n",
    "\n",
    "#conversão do texto para \n",
    "x_test = text2vector(Docs)\n",
    "\n",
    "\n",
    "# resultado da classificação multi-classe\n",
    "y_test_multi = multiClassify(x_test)\n",
    "\n",
    "\n",
    "# resultado da classificação binário\n",
    "y_test_bin = binClassify(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções complementares (necessárias para as de teste) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retorna o tipo de penalização do discriminante logistico\n",
    "def __get_penalty(value):\n",
    "\n",
    "\tif value == 1:\n",
    "\t\treturn 'l1'\n",
    "\n",
    "\telif value == 2:\n",
    "\t\treturn 'elasticnet'\n",
    "\n",
    "\telse:\n",
    "\t\treturn 'l2'\n",
    "\n",
    "\n",
    "#função que retorna o valor do class_weight do discriminante logistico\n",
    "def __get_balance_value(flag):\n",
    "\n",
    "\tif flag:\n",
    "\t\treturn 'balanced'\n",
    "\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "#função que retorna o valor da multi classe do discriminante logistico\n",
    "def __get_multi_class(value):\n",
    "\n",
    "\tif value == 1:\n",
    "\t\treturn 'auto'\n",
    "\n",
    "\telif value == 2:\n",
    "\t\treturn 'multinomial'\n",
    "\n",
    "\telse:\n",
    "\t\treturn 'ovr'\n",
    "\n",
    "\t\n",
    "#retorna o tipo de solver do discriminante logistico\n",
    "def __get_solver(value,penalty):\n",
    "\n",
    "\t\n",
    "\tif value == 1 and penalty != 1 and penalty != 2:\n",
    "\t\treturn 'lbfgs'\n",
    "\n",
    "\telif value == 2 and penalty != 1 and penalty != 2:\n",
    "\t\treturn 'sag'\n",
    "\n",
    "\telif value == 3 and penalty != 1 and penalty != 2:\n",
    "\t\treturn 'newton-cg'\n",
    "\n",
    "\telse:\n",
    "\t\treturn 'saga'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de teste dos parâmetros do discriminante logistico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função imprime na consola os scores para vários parâmetros do discriminante logistico\n",
    "#@args\n",
    "#-> data_train -> dados treino após ser feito o transform do tf-idf\n",
    "#-> y_train -> true class do treino \n",
    "#-> data_test -> dados teste após ser feito o transform do tf-idf\n",
    "#-> y_test -> true class do test\n",
    "#-> penalty -> 1 -> auto, 2 -> multinomial, outro -> ovr\n",
    "#-> solver -> 1 -> lbfgs, 2 -> sag, 3 -> sag, 4 -> newton-cg, outro -> saga\n",
    "#-> c -> valor para a regidez da penalização\n",
    "#-> balance -> True para classificar baseado nos pesos das classes\n",
    "#-> multi_class -> 1 -> auto, 2 -> multinomial, outro -> ovr\n",
    "def test_logistic_regression(data_train, y_train, data_test, y_test, penalty, solver,max_iter, c, balance, multi_class):\n",
    "\n",
    "\n",
    "\tprint(\"penalização: \"+__get_penalty(penalty)+\", resolver: \"+__get_solver(solver,penalty)+\", valor de C: \"+str(c)+\", balanceamento: \"+str(__get_balance_value(balance))+\", multi-classe: \"+__get_multi_class(multi_class))\n",
    "\t\n",
    "\tif penalty == 2:\n",
    "\t\tlogReg = LogisticRegression(penalty=__get_penalty(penalty),solver=__get_solver(solver,penalty),max_iter=1000,\n",
    "\t\t\tC=c,class_weight=__get_balance_value(balance),multi_class=__get_multi_class(multi_class),l1_ratio=0.5)\n",
    "\n",
    "\telse:\n",
    "\t\tlogReg = LogisticRegression(penalty=__get_penalty(penalty),solver=__get_solver(solver,penalty),max_iter=1000,\n",
    "\t\t\tC=c,class_weight=__get_balance_value(balance),multi_class=__get_multi_class(multi_class))\n",
    "\n",
    "\n",
    "\tlogReg.fit(data_train,y_train)\n",
    "\tprint(round(logReg.score(data_train,y_train),4))\n",
    "\tprint(round(logReg.score(data_test,y_test),4),\"\\n\")\n",
    "    \n",
    "    \n",
    "#função imprime na consola os scores e os coeficientes utilizados para vários parâmetros do discriminante logistico\n",
    "def test_logistic_regression_coef(data_train, y_train, data_test, y_test, penalty, solver,max_iter, c, balance, multi_class):\n",
    "\n",
    "\n",
    "\tprint(\"penalização: \"+__get_penalty(penalty)+\", resolver: \"+__get_solver(solver,penalty)+\", valor de C: \"+str(c)+\", balanceamento: \"+str(__get_balance_value(balance))+\", multi-classe: \"+__get_multi_class(multi_class))\n",
    "\t\n",
    "\n",
    "\tif penalty == 2:\n",
    "\t\tlogReg = LogisticRegression(penalty=__get_penalty(penalty),solver=__get_solver(solver,penalty),max_iter=1000,\n",
    "\t\t\tC=c,class_weight=__get_balance_value(balance),multi_class=__get_multi_class(multi_class),l1_ratio=0.5)\n",
    "\n",
    "\telse:\n",
    "\t\tlogReg = LogisticRegression(penalty=__get_penalty(penalty),solver=__get_solver(solver,penalty),max_iter=1000,\n",
    "\t\t\tC=c,class_weight=__get_balance_value(balance),multi_class=__get_multi_class(multi_class))\n",
    "\n",
    "\n",
    "\tlogReg.fit(data_train,y_train)\n",
    "\n",
    "\tprint(round(logReg.score(data_train,y_train),4))\n",
    "\tprint(round(logReg.score(data_test,y_test),4))\n",
    "\tprint(\"Coeficientes utilizados: \"+str(np.sum(logReg.coef_.squeeze() != 0)),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de teste do classificador Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para testar os parâmetros do classificador ridge\n",
    "#@args\n",
    "#-> data_train -> dados treino após ser feito o transform do tf-idf\n",
    "#-> y_train -> true class do treino \n",
    "#-> data_test -> dados teste após ser feito o transform do tf-idf\n",
    "#-> y_test -> true class do test\n",
    "#-> alpha -> força da regularização\n",
    "#-> fit_intercept -> Se for falso não será utilizado interceção do modelo nos calculos\n",
    "#-> normalize -> Se for True o regressor é normalizado antes de ser calculado, por subtração da média pela norma l2\n",
    "def test_ridge(data_train, y_train, data_test, y_test, alpha, fit_intercept,normalize):\n",
    "\n",
    "\n",
    "\tprint(\"Alpha: \"+str(alpha)+\", fit_intercept: \"+str(fit_intercept)+\", normalize: \"+str(normalize))\n",
    "\t\n",
    "\n",
    "\tlinReg = RidgeClassifier(alpha=alpha,fit_intercept=fit_intercept,normalize=normalize)\n",
    "\n",
    "\n",
    "\tlinReg.fit(data_train,y_train)\n",
    "\n",
    "\tprint(\"score treino:\",round(linReg.score(data_train,y_train),4))\n",
    "\tprint(\"score teste:\",round(linReg.score(data_test,y_test),4))\n",
    "\tprint(\"Coeficientes utilizados: \"+str(np.sum(linReg.coef_.squeeze() != 0)),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes realizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes para classificação multiclasse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data é as criticas\n",
    "#target o resultado das classificações\n",
    "dictionary =pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "\n",
    "#os dados já vêm em strings unicode\n",
    "X = dictionary.data\n",
    "\n",
    "#valores estão baralhados\n",
    "y = dictionary.target\n",
    "\n",
    "#divisão em treino e teste com uma aleatoridade sempre igual(para os resultados serem sempre constantes)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True,random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza dos dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2º argumento -> true caso os dados seja uma string de bytes\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2º argumento -> 1 - Porter Stemmer, 2 -> Lancaster Stemmer, outro -> Snowball Stemmer\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representação em tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2º argumento -> tamanho minimo da palavra\n",
    "#3º argumento -> número de vezes minima palavra aparecer\n",
    "#4º argumento -> n-gramas (só funciona normal e bi-grama)\n",
    "tf_idf = convert_str_to_tf_idf(x_train,3,1,1)\n",
    "\n",
    "\n",
    "#1º argumento -> instância de td-idf\n",
    "#2º argumento -> dados\n",
    "data_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "data_test = convert_to_sparce_matrix(tf_idf, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para diferentes penalizações e valores de C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalização: l1, resolver: saga, valor de C: 0.5, balanceamento: None, multi-classe: multinomial\n",
      "0.4815\n",
      "0.4473\n",
      "Coeficientes utilizados: 1718 \n",
      "\n",
      "penalização: elasticnet, resolver: saga, valor de C: 0.5, balanceamento: None, multi-classe: multinomial\n",
      "0.4967\n",
      "0.4515\n",
      "Coeficientes utilizados: 5921 \n",
      "\n",
      "penalização: l2, resolver: saga, valor de C: 0.5, balanceamento: None, multi-classe: multinomial\n",
      "0.5953\n",
      "0.4583\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 1, balanceamento: None, multi-classe: multinomial\n",
      "0.5288\n",
      "0.4524\n",
      "Coeficientes utilizados: 4414 \n",
      "\n",
      "penalização: elasticnet, resolver: saga, valor de C: 1, balanceamento: None, multi-classe: multinomial\n",
      "0.5605\n",
      "0.458\n",
      "Coeficientes utilizados: 14974 \n",
      "\n",
      "penalização: l2, resolver: saga, valor de C: 1, balanceamento: None, multi-classe: multinomial\n",
      "0.67\n",
      "0.4585\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 1.5, balanceamento: None, multi-classe: multinomial\n",
      "0.5816\n",
      "0.4484\n",
      "Coeficientes utilizados: 7810 \n",
      "\n",
      "penalização: elasticnet, resolver: saga, valor de C: 1.5, balanceamento: None, multi-classe: multinomial\n",
      "0.6164\n",
      "0.4561\n",
      "Coeficientes utilizados: 24537 \n",
      "\n",
      "penalização: l2, resolver: saga, valor de C: 1.5, balanceamento: None, multi-classe: multinomial\n",
      "0.7163\n",
      "0.4594\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 2, balanceamento: None, multi-classe: multinomial\n",
      "0.6322\n",
      "0.4416\n",
      "Coeficientes utilizados: 11443 \n",
      "\n",
      "penalização: elasticnet, resolver: saga, valor de C: 2, balanceamento: None, multi-classe: multinomial\n",
      "0.6636\n",
      "0.4521\n",
      "Coeficientes utilizados: 33532 \n",
      "\n",
      "penalização: l2, resolver: saga, valor de C: 2, balanceamento: None, multi-classe: multinomial\n",
      "0.7506\n",
      "0.4562\n",
      "Coeficientes utilizados: 193752 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#valor de c para testar a classificação\n",
    "c_values = [.5,1,1.5,2]\n",
    "\n",
    "\n",
    "for i in range(len(c_values)):\n",
    "\n",
    "\tc = c_values[i]\n",
    "\n",
    "\t#testar a função para l1, solver = saga, class_weight = None, multi_class = multinomial\n",
    "\ttest_logistic_regression_coef(data_train,y_train,data_test,y_test,1,0,1000,c,False,2)\n",
    "\n",
    "\t#testar a função para elasticinet, l1_ratio=0.5, solver = saga, class_weight = None, multi_class = multinomial\n",
    "\ttest_logistic_regression_coef(data_train,y_train,data_test,y_test,2,0,1000,c,False,2)\n",
    "\n",
    "\t#testar a função para l2, solver = saga, class_weight = None, multi_class = multinomial\n",
    "\ttest_logistic_regression_coef(data_train,y_train,data_test,y_test,3,0,1000,c,False,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para balançeamento das classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalização: l1, resolver: saga, valor de C: 0.1, balanceamento: None, multi-classe: multinomial\n",
      "0.4017\n",
      "0.3987 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.1, balanceamento: balanced, multi-classe: multinomial\n",
      "0.3751\n",
      "0.3644 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.5, balanceamento: None, multi-classe: multinomial\n",
      "0.4816\n",
      "0.4475 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.5, balanceamento: balanced, multi-classe: multinomial\n",
      "0.4623\n",
      "0.4122 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 1, balanceamento: None, multi-classe: multinomial\n",
      "0.5288\n",
      "0.4523 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 1, balanceamento: balanced, multi-classe: multinomial\n",
      "0.5231\n",
      "0.4164 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 2, balanceamento: None, multi-classe: multinomial\n",
      "0.6322\n",
      "0.4416 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 2, balanceamento: balanced, multi-classe: multinomial\n",
      "0.6418\n",
      "0.4062 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_values = [.1,.5,1,2]\n",
    "\n",
    "for i in range(len(c_values)):\n",
    "\n",
    "\tc = c_values[i]\n",
    "\n",
    "\t#testar o parâmetro class_weight para None\n",
    "\ttest_logistic_regression(data_train,y_train,data_test,y_test,1,0,1000,c,False,2)\n",
    "\n",
    "\t#testar o parâmetro class_weight para 'balanced'\n",
    "\ttest_logistic_regression(data_train,y_train,data_test,y_test,1,0,1000,c,True,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Teste para o parâmetro multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalização: l1, resolver: saga, valor de C: 0.1, balanceamento: None, multi-classe: auto\n",
      "0.4018\n",
      "0.3987 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.1, balanceamento: None, multi-classe: multinomial\n",
      "0.4017\n",
      "0.3987 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.1, balanceamento: None, multi-classe: ovr\n",
      "0.3961\n",
      "0.3914 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.5, balanceamento: None, multi-classe: auto\n",
      "0.4816\n",
      "0.4474 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.5, balanceamento: None, multi-classe: multinomial\n",
      "0.4817\n",
      "0.4475 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.5, balanceamento: None, multi-classe: ovr\n",
      "0.4776\n",
      "0.4446 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 1, balanceamento: None, multi-classe: auto\n",
      "0.5289\n",
      "0.4524 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 1, balanceamento: None, multi-classe: multinomial\n",
      "0.5288\n",
      "0.4524 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 1, balanceamento: None, multi-classe: ovr\n",
      "0.5237\n",
      "0.451 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 2, balanceamento: None, multi-classe: auto\n",
      "0.6322\n",
      "0.4416 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 2, balanceamento: None, multi-classe: multinomial\n",
      "0.6322\n",
      "0.4416 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 2, balanceamento: None, multi-classe: ovr\n",
      "0.6207\n",
      "0.4485 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(c_values)):\n",
    "\n",
    "\tc = c_values[i]\n",
    "\n",
    "\t#testar o parâmetro multi_class para 'auto'\n",
    "\ttest_logistic_regression(data_train,y_train,data_test,y_test,1,0,1000,c,False,1)\n",
    "\n",
    "\t#testar o parâmetro multi_class para 'multinomial'\n",
    "\ttest_logistic_regression(data_train,y_train,data_test,y_test,1,0,1000,c,False,2)\n",
    "\n",
    "\t#testar o parâmetro multi_class para 'ovr'\n",
    "\ttest_logistic_regression(data_train,y_train,data_test,y_test,1,0,1000,c,False,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para o parâmetro maximo iterações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalização: l1, resolver: saga, valor de C: 0.1, balanceamento: None, multi-classe: multinomial\n",
      "0.4017\n",
      "0.3986 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.1, balanceamento: None, multi-classe: multinomial\n",
      "0.4018\n",
      "0.3987 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.5, balanceamento: None, multi-classe: multinomial\n",
      "0.4816\n",
      "0.4475 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 0.5, balanceamento: None, multi-classe: multinomial\n",
      "0.4817\n",
      "0.4473 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 1, balanceamento: None, multi-classe: multinomial\n",
      "0.5289\n",
      "0.4524 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 1, balanceamento: None, multi-classe: multinomial\n",
      "0.5288\n",
      "0.4524 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 2, balanceamento: None, multi-classe: multinomial\n",
      "0.6322\n",
      "0.4416 \n",
      "\n",
      "penalização: l1, resolver: saga, valor de C: 2, balanceamento: None, multi-classe: multinomial\n",
      "0.6322\n",
      "0.4416 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testar os diferentes valores de máxima iterações\n",
    "for i in range(len(c_values)):\n",
    "\n",
    "\tc = c_values[i]\n",
    "\n",
    "\t#testar o parâmetro max_iter para 1000\n",
    "\ttest_logistic_regression(data_train,y_train,data_test,y_test,1,0,1000,c,False,2)\n",
    "\n",
    "\t#testar o parâmetro max_iter para 10000\n",
    "\ttest_logistic_regression(data_train,y_train,data_test,y_test,1,0,10000,c,False,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para diferentes stemmings para diferentes valores de C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valor de C: 0.1\n",
      "stemming: 1\n",
      "0.4022\n",
      "0.3988\n",
      "stemming: 2\n",
      "0.40323333333333333\n",
      "0.3987\n",
      "stemming: 3\n",
      "0.40323333333333333\n",
      "0.3987\n",
      "valor de C: 0.5\n",
      "stemming: 1\n",
      "0.48036666666666666\n",
      "0.4469\n",
      "stemming: 2\n",
      "0.47763333333333335\n",
      "0.4471\n",
      "stemming: 3\n",
      "0.4779\n",
      "0.4471\n",
      "valor de C: 1\n",
      "stemming: 1\n",
      "0.5284666666666666\n",
      "0.4497\n",
      "stemming: 2\n",
      "0.529\n",
      "0.4498\n",
      "stemming: 3\n",
      "0.529\n",
      "0.4501\n",
      "valor de C: 2\n",
      "stemming: 1\n",
      "0.6198333333333333\n",
      "0.4436\n",
      "stemming: 2\n",
      "0.6194333333333333\n",
      "0.4437\n",
      "stemming: 3\n",
      "0.6195\n",
      "0.4438\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(c_values)):\n",
    "\n",
    "\tprint(\"valor de C:\",c_values[i])\n",
    "\n",
    "\tfor x in range(1,4):\n",
    "\n",
    "\t\tprint(\"stemming: \"+str(x))\n",
    "\n",
    "\t\t#2º argumento -> 1 - Porter Stemmer, 2 -> Lancaster Stemmer, outro -> Snowball Stemmer\n",
    "\t\tx_train = stemming(x_train,x)\n",
    "\n",
    "\t\tx_test = stemming(x_test,x)\n",
    "\n",
    "\n",
    "\t\ttf_idf = convert_str_to_tf_idf(x_train,3,1,1)\n",
    "\n",
    "\n",
    "\t\t#1º argumento -> instância de td-idf\n",
    "\t\t#2º argumento -> dados\n",
    "\t\tdata_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "\t\tdata_test = convert_to_sparce_matrix(tf_idf, x_test)\n",
    "\n",
    "\n",
    "\t\tlogReg = LogisticRegression(penalty='l1',solver='saga',max_iter=1000,C=c_values[i],tol=1e-3,multi_class='multinomial')\n",
    "\t\tlogReg.fit(data_train,y_train)\n",
    "\n",
    "\n",
    "\t\tprint(logReg.score(data_train,y_train))\n",
    "\t\tprint(logReg.score(data_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Teste para os parâmetros do tf-idf e tamanho dos tokens para n-gramas = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 1\n",
      "1277898\n",
      "0.5033333333333333\n",
      "0.4684 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 2\n",
      "1329250\n",
      "0.46236666666666665\n",
      "0.4307 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 3\n",
      "1428136\n",
      "0.45876666666666666\n",
      "0.427 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 4\n",
      "1495784\n",
      "0.4514\n",
      "0.4196 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 5\n",
      "1169413\n",
      "0.4337\n",
      "0.3933 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 1\n",
      "407593\n",
      "0.5101333333333333\n",
      "0.4706 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 2\n",
      "414140\n",
      "0.4723\n",
      "0.4299 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 3\n",
      "405353\n",
      "0.4688\n",
      "0.4289 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 4\n",
      "374830\n",
      "0.4653333333333333\n",
      "0.4205 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 5\n",
      "247655\n",
      "0.4509666666666667\n",
      "0.399 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 1\n",
      "254128\n",
      "0.5136\n",
      "0.4711 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 2\n",
      "255492\n",
      "0.4757\n",
      "0.4312 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 3\n",
      "239330\n",
      "0.47383333333333333\n",
      "0.4292 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 4\n",
      "206230\n",
      "0.47176666666666667\n",
      "0.4215 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 5\n",
      "125870\n",
      "0.4593333333333333\n",
      "0.4033 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 1\n",
      "189587\n",
      "0.5146333333333334\n",
      "0.4711 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 2\n",
      "189224\n",
      "0.4784333333333333\n",
      "0.4311 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 3\n",
      "173170\n",
      "0.477\n",
      "0.4294 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 4\n",
      "143008\n",
      "0.47583333333333333\n",
      "0.4241 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 5\n",
      "83037\n",
      "0.46486666666666665\n",
      "0.4046 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 1\n",
      "152842\n",
      "0.5169666666666667\n",
      "0.4713 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 2\n",
      "151901\n",
      "0.4803\n",
      "0.4315 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 3\n",
      "136527\n",
      "0.4785\n",
      "0.4307 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 4\n",
      "109213\n",
      "0.47796666666666665\n",
      "0.4228 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 5\n",
      "61435\n",
      "0.46936666666666665\n",
      "0.4058 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sizeWord in range(1,6):\n",
    "\n",
    "\tfor appearTexts in range(1,6):\n",
    "\n",
    "\t\tprint(\"Tamanho da palavra:\",sizeWord)\n",
    "\n",
    "\t\tprint(\"Quantas vezes aparece a palavra:\",appearTexts)\n",
    "\n",
    "\n",
    "\t\ttf_idf = convert_str_to_tf_idf(x_train,sizeWord,appearTexts,2)\n",
    "\t\ttokens = tf_idf.get_feature_names()\n",
    "\t\tprint(len(tokens))\n",
    "\n",
    "\n",
    "\t\t#1º argumento -> instância de td-idf\n",
    "\t\t#2º argumento -> dados\n",
    "\t\tdata_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "\t\tdata_test = convert_to_sparce_matrix(tf_idf, x_test)\n",
    "\n",
    "\t\tlogReg = LogisticRegression(penalty='l1',solver='saga',max_iter=1000,C=1,tol=1e-3,multi_class='multinomial')\n",
    "\t\tlogReg.fit(data_train,y_train)\n",
    "\n",
    "\n",
    "\t\tprint(logReg.score(data_train,y_train))\n",
    "\t\tprint(logReg.score(data_test,y_test),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Teste para os parâmetros do tf-idf e tamanho dos tokens para n-gramas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 1\n",
      "53673\n",
      "0.5262\n",
      "0.4518 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 2\n",
      "53632\n",
      "0.5041666666666667\n",
      "0.4269 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 3\n",
      "52967\n",
      "0.5051\n",
      "0.426 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 4\n",
      "50359\n",
      "0.5032666666666666\n",
      "0.4174 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 5\n",
      "44277\n",
      "0.48793333333333333\n",
      "0.4054 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 1\n",
      "30486\n",
      "0.5280666666666667\n",
      "0.4519 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 2\n",
      "30447\n",
      "0.5062\n",
      "0.4272 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 3\n",
      "29932\n",
      "0.5073666666666666\n",
      "0.4267 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 4\n",
      "28280\n",
      "0.5053333333333333\n",
      "0.4185 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 5\n",
      "24280\n",
      "0.4916\n",
      "0.4069 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 1\n",
      "24219\n",
      "0.5287666666666667\n",
      "0.4523 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 2\n",
      "24181\n",
      "0.5071333333333333\n",
      "0.4264 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 3\n",
      "23740\n",
      "0.5082\n",
      "0.4257 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 4\n",
      "22389\n",
      "0.5068\n",
      "0.4198 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 5\n",
      "19020\n",
      "0.4926\n",
      "0.4053 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 1\n",
      "21014\n",
      "0.5294\n",
      "0.4527 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 2\n",
      "20976\n",
      "0.5079\n",
      "0.4267 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 3\n",
      "20572\n",
      "0.5083\n",
      "0.4257 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 4\n",
      "19397\n",
      "0.5072333333333333\n",
      "0.4185 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 5\n",
      "16378\n",
      "0.4941333333333333\n",
      "0.4047 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 1\n",
      "18752\n",
      "0.5300666666666667\n",
      "0.4526 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 2\n",
      "18715\n",
      "0.5081333333333333\n",
      "0.4266 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 3\n",
      "18349\n",
      "0.5091\n",
      "0.4251 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 4\n",
      "17283\n",
      "0.508\n",
      "0.4187 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 5\n",
      "14521\n",
      "0.49506666666666665\n",
      "0.4045 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sizeWord in range(1,6):\n",
    "\n",
    "\tfor appearTexts in range(1,6):\n",
    "\n",
    "\t\tprint(\"Tamanho da palavra:\",sizeWord)\n",
    "\n",
    "\t\tprint(\"Quantas vezes aparece a palavra:\",appearTexts)\n",
    "\n",
    "\n",
    "\t\ttf_idf = convert_str_to_tf_idf(x_train,sizeWord,appearTexts,1)\n",
    "\t\ttokens = tf_idf.get_feature_names()\n",
    "\t\tprint(len(tokens))\n",
    "\n",
    "\n",
    "\t\t#1º argumento -> instância de td-idf\n",
    "\t\t#2º argumento -> dados\n",
    "\t\tdata_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "\t\tdata_test = convert_to_sparce_matrix(tf_idf, x_test)\n",
    "\n",
    "\t\tlogReg = LogisticRegression(penalty='l1',solver='saga',max_iter=1000,C=1,tol=1e-3,multi_class='multinomial')\n",
    "\t\tlogReg.fit(data_train,y_train)\n",
    "\n",
    "\n",
    "\t\tprint(logReg.score(data_train,y_train))\n",
    "\t\tprint(logReg.score(data_test,y_test),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para saber quais os melhores parâmetros do discriminante logistico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=3)]: Done 108 out of 108 | elapsed: 13.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.5324666666666666\n",
      "acertos no teste:  0.4536\n",
      "Parâmetros multi-classe: ovr C: 1.1 penalty: l1\n"
     ]
    }
   ],
   "source": [
    "penaltyTest = ['l1','l2']\n",
    "multiClass = ['multinomial','ovr','auto']\n",
    "cList = [0.1,0.5,1,1.1,1.2,1.3,1.4,1.5,2]\n",
    "grelha={'multi_class':multiClass,'C':cList,'penalty':penaltyTest}\n",
    "\n",
    "gSearch = GridSearchCV(LogisticRegression(solver='saga',max_iter=1000),param_grid=grelha,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(data_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(data_train,y_train))\n",
    "print('acertos no teste: ',svm.score(data_test,y_test))\n",
    "print('Parâmetros multi-classe:',par['multi_class'],'C:',par['C'],'penalty:',par['penalty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste para os melhores parâmetros para uma dada semente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste sobre os melhores parâmetros de tf-idf e discriminante logistico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 192 candidates, totalling 384 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed: 32.9min\n",
      "[Parallel(n_jobs=3)]: Done 384 out of 384 | elapsed: 105.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.5958666666666667\n",
      "acertos no teste:  0.4822\n",
      "Parâmetros: {'logistic__C': 2, 'logistic__multi_class': 'ovr', 'logistic__penalty': 'l1', 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b', r'\\b\\w\\w\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [0.5,1,1.5,2],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 18.3min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed: 111.1min\n",
      "[Parallel(n_jobs=3)]: Done 288 out of 288 | elapsed: 191.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.5958666666666667\n",
      "acertos no teste:  0.4822\n",
      "Parâmetros: {'logistic__C': 2, 'logistic__multi_class': 'ovr', 'logistic__penalty': 'l1', 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1, 3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [0.5,1,1.1,1.2,1.5,2],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 37.8min\n",
      "[Parallel(n_jobs=3)]: Done  64 out of  64 | elapsed: 54.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.5863666666666667\n",
      "acertos no teste:  0.4829\n",
      "Parâmetros: {'logistic__C': 1.9, 'logistic__multi_class': 'ovr', 'logistic__penalty': 'l1', 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [1.9,2,2.1,2.2],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 32.0min\n",
      "[Parallel(n_jobs=3)]: Done  64 out of  64 | elapsed: 47.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.5864\n",
      "acertos no teste:  0.4829\n",
      "Parâmetros: {'logistic__C': 1.9, 'logistic__multi_class': 'ovr', 'logistic__penalty': 'l1', 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [1.6,1.7,1.8,1.9],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes para sementes aleatórias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "y = dictionary.target\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True)\n",
    "\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  48 out of  48 | elapsed: 38.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.5974333333333334\n",
      "acertos no teste:  0.4782\n",
      "Parâmetros: {'logistic__C': 1.9, 'logistic__multi_class': 'ovr', 'logistic__penalty': 'l1', 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [1.8,1.9,2],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 44.0min\n",
      "[Parallel(n_jobs=3)]: Done  96 out of  96 | elapsed: 91.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.5952\n",
      "acertos no teste:  0.4835\n",
      "Parâmetros: {'logistic__C': 2, 'logistic__multi_class': 'ovr', 'logistic__penalty': 'l1', 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1, 3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b',r'\\b\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [1.9,2],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes para classificação binária "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "y = dictionary.target\n",
    "\n",
    "y = convert_indexes_into_binary(y)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2º argumento -> true caso os dados seja uma string de bytes\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "\n",
    "#2º argumento -> 1 - Porter Stemmer, 2 -> Lancaster Stemmer, outro -> Snowball Stemmer\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor tamanho minimo: 1\n",
      "Aparecimento dos textos: 1\n",
      "Tamanho dos tokens:  53673\n",
      "0.9029333333333334\n",
      "0.8819\n",
      "Valor tamanho minimo: 1\n",
      "Aparecimento dos textos: 2\n",
      "Tamanho dos tokens:  53632\n",
      "0.9004333333333333\n",
      "0.8829\n",
      "Valor tamanho minimo: 1\n",
      "Aparecimento dos textos: 3\n",
      "Tamanho dos tokens:  52967\n",
      "0.8979333333333334\n",
      "0.8816\n",
      "Valor tamanho minimo: 1\n",
      "Aparecimento dos textos: 4\n",
      "Tamanho dos tokens:  50359\n",
      "0.8951666666666667\n",
      "0.8723\n",
      "Valor tamanho minimo: 1\n",
      "Aparecimento dos textos: 5\n",
      "Tamanho dos tokens:  44277\n",
      "0.8784333333333333\n",
      "0.854\n",
      "Valor tamanho minimo: 2\n",
      "Aparecimento dos textos: 1\n",
      "Tamanho dos tokens:  30486\n",
      "0.9031333333333333\n",
      "0.8818\n",
      "Valor tamanho minimo: 2\n",
      "Aparecimento dos textos: 2\n",
      "Tamanho dos tokens:  30447\n",
      "0.9008333333333334\n",
      "0.8825\n",
      "Valor tamanho minimo: 2\n",
      "Aparecimento dos textos: 3\n",
      "Tamanho dos tokens:  29932\n",
      "0.8991333333333333\n",
      "0.8811\n",
      "Valor tamanho minimo: 2\n",
      "Aparecimento dos textos: 4\n",
      "Tamanho dos tokens:  28280\n",
      "0.8954333333333333\n",
      "0.8728\n",
      "Valor tamanho minimo: 2\n",
      "Aparecimento dos textos: 5\n",
      "Tamanho dos tokens:  24280\n",
      "0.8794333333333333\n",
      "0.8549\n",
      "Valor tamanho minimo: 3\n",
      "Aparecimento dos textos: 1\n",
      "Tamanho dos tokens:  24219\n",
      "0.9034333333333333\n",
      "0.8823\n",
      "Valor tamanho minimo: 3\n",
      "Aparecimento dos textos: 2\n",
      "Tamanho dos tokens:  24181\n",
      "0.9012\n",
      "0.8826\n",
      "Valor tamanho minimo: 3\n",
      "Aparecimento dos textos: 3\n",
      "Tamanho dos tokens:  23740\n",
      "0.8991\n",
      "0.8811\n",
      "Valor tamanho minimo: 3\n",
      "Aparecimento dos textos: 4\n",
      "Tamanho dos tokens:  22389\n",
      "0.8955\n",
      "0.8731\n",
      "Valor tamanho minimo: 3\n",
      "Aparecimento dos textos: 5\n",
      "Tamanho dos tokens:  19020\n",
      "0.8796333333333334\n",
      "0.8559\n",
      "Valor tamanho minimo: 4\n",
      "Aparecimento dos textos: 1\n",
      "Tamanho dos tokens:  21014\n",
      "0.9033333333333333\n",
      "0.8823\n",
      "Valor tamanho minimo: 4\n",
      "Aparecimento dos textos: 2\n",
      "Tamanho dos tokens:  20976\n",
      "0.9013\n",
      "0.8825\n",
      "Valor tamanho minimo: 4\n",
      "Aparecimento dos textos: 3\n",
      "Tamanho dos tokens:  20572\n",
      "0.8993333333333333\n",
      "0.8812\n",
      "Valor tamanho minimo: 4\n",
      "Aparecimento dos textos: 4\n",
      "Tamanho dos tokens:  19397\n",
      "0.8958\n",
      "0.8732\n",
      "Valor tamanho minimo: 4\n",
      "Aparecimento dos textos: 5\n",
      "Tamanho dos tokens:  16378\n",
      "0.8795333333333333\n",
      "0.8561\n",
      "Valor tamanho minimo: 5\n",
      "Aparecimento dos textos: 1\n",
      "Tamanho dos tokens:  18752\n",
      "0.9034666666666666\n",
      "0.8819\n",
      "Valor tamanho minimo: 5\n",
      "Aparecimento dos textos: 2\n",
      "Tamanho dos tokens:  18715\n",
      "0.9015\n",
      "0.8827\n",
      "Valor tamanho minimo: 5\n",
      "Aparecimento dos textos: 3\n",
      "Tamanho dos tokens:  18349\n",
      "0.8993666666666666\n",
      "0.8812\n",
      "Valor tamanho minimo: 5\n",
      "Aparecimento dos textos: 4\n",
      "Tamanho dos tokens:  17283\n",
      "0.8960666666666667\n",
      "0.8737\n",
      "Valor tamanho minimo: 5\n",
      "Aparecimento dos textos: 5\n",
      "Tamanho dos tokens:  14521\n",
      "0.8802\n",
      "0.8563\n"
     ]
    }
   ],
   "source": [
    "#Para os diferentes valores de min_df, token_pattern para n-grams = 1\n",
    "for i in range(1,6):\n",
    "\n",
    "\tfor j in range(1,6):\n",
    "\n",
    "\t\tprint(\"Valor tamanho minimo:\",i)\n",
    "\n",
    "\t\tprint(\"Aparecimento dos textos:\",j)\n",
    "\n",
    "\t\ttf_idf = convert_str_to_tf_idf(x_train,i,j,1)\n",
    "\t\ttokens = tf_idf.get_feature_names()\n",
    "\t\tprint(\"Tamanho dos tokens: \",len(tokens))\n",
    "\n",
    "\t\tdata_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "\t\tdata_test = convert_to_sparce_matrix(tf_idf, x_test)\n",
    "\n",
    "\n",
    "\t\tlogReg = LogisticRegression(penalty='l1',solver='saga',max_iter=1000,C=1,tol=1e-3,multi_class='ovr')\n",
    "\t\tlogReg.fit(data_train,y_train)\n",
    "\n",
    "\t\tprint(logReg.score(data_train,y_train))\n",
    "\t\tprint(logReg.score(data_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste melhores parâmetros para uma dada semente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=3)]: Done 120 out of 120 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9563666666666667\n",
      "acertos no teste:  0.8977\n",
      "Parâmetros multi-classe: multinomial C: 2.1 penalty: l2\n"
     ]
    }
   ],
   "source": [
    "#2º argumento -> tamanho minimo da palavra\n",
    "#3º argumento -> número de vezes minima palavra aparecer\n",
    "#4º argumento -> n-gramas (só funciona normal e bi-grama)\n",
    "tf_idf = convert_str_to_tf_idf(x_train,3,1,1)\n",
    "tokens = tf_idf.get_feature_names()\n",
    "\n",
    "\n",
    "#1º argumento -> instância de td-idf\n",
    "#2º argumento -> dados\n",
    "data_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "data_test = convert_to_sparce_matrix(tf_idf, x_test)\n",
    "\n",
    "penaltyTest = ['l1','l2']\n",
    "multiClass = ['multinomial','ovr','auto']\n",
    "cList = [0.1,0.5,1,1.5,2,2.1,2.2,2.3,2.4,2.5]\n",
    "grelha={'multi_class':multiClass,'C':cList,'penalty':penaltyTest}\n",
    "\n",
    "gSearch = GridSearchCV(LogisticRegression(solver='saga',max_iter=1000),param_grid=grelha,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(data_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(data_train,y_train))\n",
    "print('acertos no teste: ',svm.score(data_test,y_test))\n",
    "print('Parâmetros multi-classe:',par['multi_class'],'C:',par['C'],'penalty:',par['penalty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 128 candidates, totalling 256 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=3)]: Done 256 out of 256 | elapsed: 24.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9885333333333334\n",
      "acertos no teste:  0.9143\n",
      "Parâmetros: {'logistic__C': 2, 'logistic__multi_class': 'multinomial', 'logistic__penalty': 'l2', 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [0.5,1,1.5,2],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste com semente alteatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "y = dictionary.target\n",
    "\n",
    "y = convert_indexes_into_binary(y)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True)\n",
    "\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=3)]: Done  96 out of  96 | elapsed: 20.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9863333333333333\n",
      "acertos no teste:  0.914\n",
      "Parâmetros: {'logistic__C': 2, 'logistic__multi_class': 'multinomial', 'logistic__penalty': 'l2', 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [1.5,1.7,1.8,1.9,2,2.1],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=3)]: Done  96 out of  96 | elapsed: 23.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9868333333333333\n",
      "acertos no teste:  0.912\n",
      "Parâmetros: {'logistic__C': 2.1, 'logistic__multi_class': 'multinomial', 'logistic__penalty': 'l2', 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [1.5,1.7,1.8,1.9,2,2.1],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=3)]: Done  96 out of  96 | elapsed: 19.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9866333333333334\n",
      "acertos no teste:  0.9156\n",
      "Parâmetros: {'logistic__C': 2.1, 'logistic__multi_class': 'multinomial', 'logistic__penalty': 'l2', 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('logistic', logReg)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'logistic__penalty': ['l1','l2'],\n",
    "    'logistic__C': [1.5,1.7,1.8,1.9,2,2.1],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes do classificador Ridge para multi-classe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recarregamento dos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "\n",
    "y = dictionary.target\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True,random_state=0)\n",
    "\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)\n",
    "\n",
    "\n",
    "tf_idf = convert_str_to_tf_idf(x_train,3,1,1)\n",
    "\n",
    "\n",
    "data_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "data_test = convert_to_sparce_matrix(tf_idf, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para diversos valores de alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, fit_intercept: True, normalize: False\n",
      "score treino: 0.9532\n",
      "score teste: 0.3788\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 0.5, fit_intercept: True, normalize: False\n",
      "score treino: 0.8747\n",
      "score teste: 0.428\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1, fit_intercept: True, normalize: False\n",
      "score treino: 0.8141\n",
      "score teste: 0.4437\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1.1, fit_intercept: True, normalize: False\n",
      "score treino: 0.8041\n",
      "score teste: 0.4457\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1.2, fit_intercept: True, normalize: False\n",
      "score treino: 0.7955\n",
      "score teste: 0.4482\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1.3, fit_intercept: True, normalize: False\n",
      "score treino: 0.7875\n",
      "score teste: 0.4494\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1.4, fit_intercept: True, normalize: False\n",
      "score treino: 0.7794\n",
      "score teste: 0.4504\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1.5, fit_intercept: True, normalize: False\n",
      "score treino: 0.7719\n",
      "score teste: 0.4517\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1.6, fit_intercept: True, normalize: False\n",
      "score treino: 0.7655\n",
      "score teste: 0.4528\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1.7, fit_intercept: True, normalize: False\n",
      "score treino: 0.7597\n",
      "score teste: 0.4535\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1.8, fit_intercept: True, normalize: False\n",
      "score treino: 0.7538\n",
      "score teste: 0.4552\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 1.9, fit_intercept: True, normalize: False\n",
      "score treino: 0.7484\n",
      "score teste: 0.4556\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.7423\n",
      "score teste: 0.4574\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6426\n",
      "score teste: 0.459\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 10, fit_intercept: True, normalize: False\n",
      "score treino: 0.5727\n",
      "score teste: 0.4558\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 15, fit_intercept: True, normalize: False\n",
      "score treino: 0.5382\n",
      "score teste: 0.4487\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Alpha: 20, fit_intercept: True, normalize: False\n",
      "score treino: 0.5131\n",
      "score teste: 0.4433\n",
      "Coeficientes utilizados: 193752 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_values =  [.1,.5,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,5,10,15,20]\n",
    "\n",
    "\n",
    "for i in range(len(c_values)):\n",
    "\n",
    "\ttest_ridge(data_train,y_train,data_test,y_test,c_values[i],True,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes sobre os parâmetros min_df e token_pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 1\n",
      "53673\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6551\n",
      "score teste: 0.4584\n",
      "Coeficientes utilizados: 429384 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 2\n",
      "53632\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6426\n",
      "score teste: 0.4406\n",
      "Coeficientes utilizados: 429056 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 3\n",
      "52967\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6446\n",
      "score teste: 0.4391\n",
      "Coeficientes utilizados: 423736 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 4\n",
      "50359\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6478\n",
      "score teste: 0.4322\n",
      "Coeficientes utilizados: 402872 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 5\n",
      "44277\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6351\n",
      "score teste: 0.4119\n",
      "Coeficientes utilizados: 354216 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 1\n",
      "30486\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6468\n",
      "score teste: 0.4594\n",
      "Coeficientes utilizados: 243888 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 2\n",
      "30447\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6337\n",
      "score teste: 0.4403\n",
      "Coeficientes utilizados: 243576 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 3\n",
      "29932\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6362\n",
      "score teste: 0.4373\n",
      "Coeficientes utilizados: 239456 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 4\n",
      "28280\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.64\n",
      "score teste: 0.4325\n",
      "Coeficientes utilizados: 226240 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 5\n",
      "24280\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6228\n",
      "score teste: 0.4111\n",
      "Coeficientes utilizados: 194240 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 1\n",
      "24219\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6426\n",
      "score teste: 0.459\n",
      "Coeficientes utilizados: 193752 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 2\n",
      "24181\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6288\n",
      "score teste: 0.4402\n",
      "Coeficientes utilizados: 193448 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 3\n",
      "23740\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6317\n",
      "score teste: 0.4369\n",
      "Coeficientes utilizados: 189920 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 4\n",
      "22389\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6337\n",
      "score teste: 0.4315\n",
      "Coeficientes utilizados: 179112 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 5\n",
      "19020\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6161\n",
      "score teste: 0.4105\n",
      "Coeficientes utilizados: 152160 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 1\n",
      "21014\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6389\n",
      "score teste: 0.4585\n",
      "Coeficientes utilizados: 168112 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 2\n",
      "20976\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.625\n",
      "score teste: 0.4407\n",
      "Coeficientes utilizados: 167808 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 3\n",
      "20572\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6275\n",
      "score teste: 0.4364\n",
      "Coeficientes utilizados: 164576 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 4\n",
      "19397\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6294\n",
      "score teste: 0.4303\n",
      "Coeficientes utilizados: 155176 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 5\n",
      "16378\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6118\n",
      "score teste: 0.4106\n",
      "Coeficientes utilizados: 131024 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 1\n",
      "18752\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6361\n",
      "score teste: 0.4581\n",
      "Coeficientes utilizados: 150016 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 2\n",
      "18715\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6222\n",
      "score teste: 0.4395\n",
      "Coeficientes utilizados: 149720 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 3\n",
      "18349\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6235\n",
      "score teste: 0.4354\n",
      "Coeficientes utilizados: 146792 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 4\n",
      "17283\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6255\n",
      "score teste: 0.4308\n",
      "Coeficientes utilizados: 138264 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 5\n",
      "14521\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.6069\n",
      "score teste: 0.4102\n",
      "Coeficientes utilizados: 116168 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sizeWord in range(1,6):\n",
    "\n",
    "\tfor appearTexts in range(1,6):\n",
    "\n",
    "\t\tprint(\"Tamanho da palavra:\",sizeWord)\n",
    "\n",
    "\t\tprint(\"Quantas vezes aparece a palavra:\",appearTexts)\n",
    "\n",
    "\n",
    "\t\ttf_idf = convert_str_to_tf_idf(x_train,sizeWord,appearTexts,1)\n",
    "\t\ttokens = tf_idf.get_feature_names()\n",
    "\t\tprint(len(tokens))\n",
    "\n",
    "\n",
    "\t\tdata_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "\t\tdata_test = convert_to_sparce_matrix(tf_idf, x_test)\n",
    "\n",
    "\n",
    "\t\ttest_ridge(data_train,y_train,data_test,y_test,5,True,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste melhores parâmetros para tf-idf e ridge para uma dada semente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 180 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=3)]: Done 360 out of 360 | elapsed: 14.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9377333333333333\n",
      "acertos no teste:  0.4829\n",
      "Parâmetros: {'ridge__alpha': 2.2, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b', r'\\b\\w\\w\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'ridge__alpha': [0.1,0.5,1,1.5,1.6,1.7,1.8,1.9,2,2,2.1,2.2,2.3,2.4,2.5]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Testes para semente aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "\n",
    "y = dictionary.target\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True)\n",
    "\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "\n",
    "#2º argumento -> 1 - Porter Stemmer, 2 -> Lancaster Stemmer, outro -> Snowball Stemmer\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 180 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=3)]: Done 360 out of 360 | elapsed: 14.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9750333333333333\n",
      "acertos no teste:  0.4855\n",
      "Parâmetros: {'ridge__alpha': 1.5, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b', r'\\b\\w\\w\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'ridge__alpha': [0.1,0.5,1,1.5,1.6,1.7,1.8,1.9,2,2,2.1,2.2,2.3,2.4,2.5]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=3)]: Done 120 out of 120 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9541\n",
      "acertos no teste:  0.4874\n",
      "Parâmetros: {'ridge__alpha': 1.9, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b', r'\\b\\w\\w\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'ridge__alpha': [1,1.5,1.6,1.7,1.8,1.9,2,2,2.1,2.2]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 96 candidates, totalling 192 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=3)]: Done 192 out of 192 | elapsed: 18.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9710333333333333\n",
      "acertos no teste:  0.4815\n",
      "Parâmetros: {'ridge__alpha': 1.7, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1,2,3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'ridge__alpha': [0,5,1,1.5,1.6,1.7,1.8,1.9,2,2,2.1,2.2]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 96 candidates, totalling 192 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=3)]: Done 192 out of 192 | elapsed: 23.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9873333333333333\n",
      "acertos no teste:  0.4794\n",
      "Parâmetros: {'ridge__alpha': 1.5, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1,2,3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'ridge__alpha': [0,5,1,1.5,1.6,1.7,1.8,1.9,2,2,2.1,2.2]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes do classificador Ridge para binário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "\n",
    "#os dados já vêm em strings unicode\n",
    "X = dictionary.data\n",
    "\n",
    "#valores estão baralhados\n",
    "y = dictionary.target\n",
    "\n",
    "y = convert_indexes_into_binary(y)\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True,random_state=0)\n",
    "\n",
    "\n",
    "#2º argumento -> true caso os dados seja uma string de bytes\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "\n",
    "#2º argumento -> 1 - Porter Stemmer, 2 -> Lancaster Stemmer, outro -> Snowball Stemmer\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)\n",
    "\n",
    "#2º argumento -> tamanho minimo da palavra\n",
    "#3º argumento -> número de vezes minima palavra aparecer\n",
    "#4º argumento -> n-gramas (só funciona normal e bi-grama)\n",
    "tf_idf = convert_str_to_tf_idf(x_train,3,1,2)\n",
    "tokens = tf_idf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para varios valores de alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, fit_intercept: True, normalize: False\n",
      "score treino: 0.9656\n",
      "score teste: 0.8245\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 0.5, fit_intercept: True, normalize: False\n",
      "score treino: 0.9513\n",
      "score teste: 0.8519\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1, fit_intercept: True, normalize: False\n",
      "score treino: 0.9412\n",
      "score teste: 0.8608\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1.1, fit_intercept: True, normalize: False\n",
      "score treino: 0.9392\n",
      "score teste: 0.8624\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1.2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9375\n",
      "score teste: 0.8625\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1.3, fit_intercept: True, normalize: False\n",
      "score treino: 0.9359\n",
      "score teste: 0.8642\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1.4, fit_intercept: True, normalize: False\n",
      "score treino: 0.9348\n",
      "score teste: 0.8651\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1.5, fit_intercept: True, normalize: False\n",
      "score treino: 0.9337\n",
      "score teste: 0.8658\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1.6, fit_intercept: True, normalize: False\n",
      "score treino: 0.9326\n",
      "score teste: 0.8654\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1.7, fit_intercept: True, normalize: False\n",
      "score treino: 0.9318\n",
      "score teste: 0.8656\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1.8, fit_intercept: True, normalize: False\n",
      "score treino: 0.9311\n",
      "score teste: 0.8661\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 1.9, fit_intercept: True, normalize: False\n",
      "score treino: 0.9303\n",
      "score teste: 0.866\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9292\n",
      "score teste: 0.8664\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 5, fit_intercept: True, normalize: False\n",
      "score treino: 0.9111\n",
      "score teste: 0.8674\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 10, fit_intercept: True, normalize: False\n",
      "score treino: 0.8955\n",
      "score teste: 0.8657\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 15, fit_intercept: True, normalize: False\n",
      "score treino: 0.8873\n",
      "score teste: 0.8626\n",
      "Coeficientes utilizados: 14521 \n",
      "\n",
      "Alpha: 20, fit_intercept: True, normalize: False\n",
      "score treino: 0.8813\n",
      "score teste: 0.8597\n",
      "Coeficientes utilizados: 14521 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_values =  [.1,.5,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,5,10,15,20]\n",
    "\n",
    "\n",
    "for i in range(len(c_values)):\n",
    "\n",
    "\ttest_ridge(data_train,y_train,data_test,y_test,c_values[i],True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 1\n",
      "53673\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9542\n",
      "score teste: 0.8987\n",
      "Coeficientes utilizados: 53673 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 2\n",
      "53632\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9533\n",
      "score teste: 0.8964\n",
      "Coeficientes utilizados: 53632 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 3\n",
      "52967\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9527\n",
      "score teste: 0.8951\n",
      "Coeficientes utilizados: 52967 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 4\n",
      "50359\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.951\n",
      "score teste: 0.8886\n",
      "Coeficientes utilizados: 50359 \n",
      "\n",
      "Tamanho da palavra: 1\n",
      "Quantas vezes aparece a palavra: 5\n",
      "44277\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9406\n",
      "score teste: 0.8699\n",
      "Coeficientes utilizados: 44277 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 1\n",
      "30486\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9514\n",
      "score teste: 0.8973\n",
      "Coeficientes utilizados: 30486 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 2\n",
      "30447\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9509\n",
      "score teste: 0.8954\n",
      "Coeficientes utilizados: 30447 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 3\n",
      "29932\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9499\n",
      "score teste: 0.8941\n",
      "Coeficientes utilizados: 29932 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 4\n",
      "28280\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9474\n",
      "score teste: 0.888\n",
      "Coeficientes utilizados: 28280 \n",
      "\n",
      "Tamanho da palavra: 2\n",
      "Quantas vezes aparece a palavra: 5\n",
      "24280\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9355\n",
      "score teste: 0.8678\n",
      "Coeficientes utilizados: 24280 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 1\n",
      "24219\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9502\n",
      "score teste: 0.8976\n",
      "Coeficientes utilizados: 24219 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 2\n",
      "24181\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9497\n",
      "score teste: 0.8949\n",
      "Coeficientes utilizados: 24181 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 3\n",
      "23740\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9482\n",
      "score teste: 0.8936\n",
      "Coeficientes utilizados: 23740 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 4\n",
      "22389\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9457\n",
      "score teste: 0.8872\n",
      "Coeficientes utilizados: 22389 \n",
      "\n",
      "Tamanho da palavra: 3\n",
      "Quantas vezes aparece a palavra: 5\n",
      "19020\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9333\n",
      "score teste: 0.8676\n",
      "Coeficientes utilizados: 19020 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 1\n",
      "21014\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9489\n",
      "score teste: 0.8971\n",
      "Coeficientes utilizados: 21014 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 2\n",
      "20976\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9483\n",
      "score teste: 0.8954\n",
      "Coeficientes utilizados: 20976 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 3\n",
      "20572\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9471\n",
      "score teste: 0.8928\n",
      "Coeficientes utilizados: 20572 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 4\n",
      "19397\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9438\n",
      "score teste: 0.8865\n",
      "Coeficientes utilizados: 19397 \n",
      "\n",
      "Tamanho da palavra: 4\n",
      "Quantas vezes aparece a palavra: 5\n",
      "16378\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.931\n",
      "score teste: 0.8666\n",
      "Coeficientes utilizados: 16378 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 1\n",
      "18752\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.948\n",
      "score teste: 0.8961\n",
      "Coeficientes utilizados: 18752 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 2\n",
      "18715\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9471\n",
      "score teste: 0.8951\n",
      "Coeficientes utilizados: 18715 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 3\n",
      "18349\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9461\n",
      "score teste: 0.8929\n",
      "Coeficientes utilizados: 18349 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 4\n",
      "17283\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9426\n",
      "score teste: 0.8869\n",
      "Coeficientes utilizados: 17283 \n",
      "\n",
      "Tamanho da palavra: 5\n",
      "Quantas vezes aparece a palavra: 5\n",
      "14521\n",
      "Alpha: 2, fit_intercept: True, normalize: False\n",
      "score treino: 0.9292\n",
      "score teste: 0.8664\n",
      "Coeficientes utilizados: 14521 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#N-gramas de 1 (sem nada)\n",
    "for sizeWord in range(1,6):\n",
    "\n",
    "\tfor appearTexts in range(1,6):\n",
    "\n",
    "\t\tprint(\"Tamanho da palavra:\",sizeWord)\n",
    "\n",
    "\t\tprint(\"Quantas vezes aparece a palavra:\",appearTexts)\n",
    "\n",
    "\n",
    "\t\ttf_idf = convert_str_to_tf_idf(x_train,sizeWord,appearTexts,1)\n",
    "\t\ttokens = tf_idf.get_feature_names()\n",
    "\t\tprint(len(tokens))\n",
    "\n",
    "\n",
    "\t\tdata_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "\t\tdata_test = convert_to_sparce_matrix(tf_idf, x_test)\n",
    "\n",
    "\n",
    "\t\ttest_ridge(data_train,y_train,data_test,y_test,2,True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=3)]: Done 200 out of 200 | elapsed:   11.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9292333333333334\n",
      "acertos no teste:  0.8664\n",
      "Parâmetros: alpha: 2 fit_intercept: True normalize: False\n"
     ]
    }
   ],
   "source": [
    "alphaList = [0.1,0.5,1,1.1,1.2,1.3,1.4,1.5,2,2.5]\n",
    "fitList = [True,False]\n",
    "normList = [True,False]\n",
    "#cList = [0.1,0.2]\n",
    "grelha={'alpha':alphaList,'fit_intercept':fitList,'normalize':normList}\n",
    "\n",
    "gSearch = GridSearchCV(RidgeClassifier(),param_grid=grelha,cv=5,verbose=1,n_jobs=3)\n",
    "gSearch.fit(data_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(data_train,y_train))\n",
    "print('acertos no teste: ',svm.score(data_test,y_test))\n",
    "print('Parâmetros: alpha:',par['alpha'],'fit_intercept:',par['fit_intercept'],'normalize:',par['normalize'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste para uma determinada semente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 180 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=3)]: Done 360 out of 360 | elapsed: 12.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9965333333333334\n",
      "acertos no teste:  0.9175\n",
      "Parâmetros: {'ridge__alpha': 1, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b', r'\\b\\w\\w\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'ridge__alpha': [0.1,0.5,1,1.5,1.6,1.7,1.8,1.9,2,2,2.1,2.2,2.3,2.4,2.5]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste para semente aleatória "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "\n",
    "y = dictionary.target\n",
    "\n",
    "y = convert_indexes_into_binary(y)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True)\n",
    "\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 180 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=3)]: Done 360 out of 360 | elapsed: 12.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9963666666666666\n",
      "acertos no teste:  0.9162\n",
      "Parâmetros: {'ridge__alpha': 1, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b', r'\\b\\w\\w\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'ridge__alpha': [0.1,0.5,1,1.5,1.6,1.7,1.8,1.9,2,2,2.1,2.2,2.3,2.4,2.5]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=3)]: Done 144 out of 144 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9998666666666667\n",
      "acertos no teste:  0.9134\n",
      "Parâmetros: {'ridge__alpha': 0.5, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1,2,3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b', r'\\b\\w\\w\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'ridge__alpha': [0.5,1,1.5,1.9,2,2.1]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=3)]: Done 144 out of 144 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9999666666666667\n",
      "acertos no teste:  0.9152\n",
      "Parâmetros: {'ridge__alpha': 0.5, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1,2,3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w\\w+\\b', r'\\b\\w\\w\\w\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'ridge__alpha': [0.5,1,1.5,1.9,2,2.1]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=3)]: Done  96 out of  96 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.997\n",
      "acertos no teste:  0.9165\n",
      "Parâmetros: {'ridge__alpha': 1, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1,2,3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'ridge__alpha': [0.5,1,1.5,1.9,2,2.1]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=3)]: Done  96 out of  96 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9999666666666667\n",
      "acertos no teste:  0.9142\n",
      "Parâmetros: {'ridge__alpha': 0.5, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier(max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1,2,3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'ridge__alpha': [0.5,1,1.5,1.9,2,2.1]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=3)]: Done  96 out of  96 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.9999333333333333\n",
      "acertos no teste:  0.9158\n",
      "Parâmetros: {'ridge__alpha': 0.5, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier(max_iter=1000)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [1,2,3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'ridge__alpha': [0.5,1,1.5,1.9,2,2.1]\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilização de PCA no pré-processamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142666666666667\n",
      "0.4579\n"
     ]
    }
   ],
   "source": [
    "dictionary = pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "\n",
    "y = dictionary.target\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True,random_state=0)\n",
    "\n",
    "\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)\n",
    "\n",
    "\n",
    "tf_idf = convert_str_to_tf_idf(x_train,3,1,1)\n",
    "\n",
    "\n",
    "data_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "data_test = convert_to_sparce_matrix(tf_idf, x_test)\n",
    "\n",
    "\n",
    "logReg = LogisticRegression(penalty='l2',solver='saga',max_iter=1000,C=2,tol=1e-3,multi_class='ovr')\n",
    "logReg.fit(data_train,y_train)\n",
    "\n",
    "print(logReg.score(data_train,y_train))\n",
    "print(logReg.score(data_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.262\n",
      "0.2607\n"
     ]
    }
   ],
   "source": [
    "dictionary = pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "\n",
    "y = dictionary.target\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True,random_state=0)\n",
    "\n",
    "\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)\n",
    "\n",
    "\n",
    "tf_idf = convert_str_to_tf_idf(x_train,3,1,1)\n",
    "\n",
    "\n",
    "data_train = convert_to_sparce_matrix(tf_idf, x_train)\n",
    "\n",
    "data_test = convert_to_sparce_matrix(tf_idf, x_test)\n",
    "\n",
    "# utilizar 5 componentes principais\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7).fit(data_train)\n",
    "data_train = svd.transform(data_train)\n",
    "data_test = svd.transform(data_test)\n",
    "\n",
    "\n",
    "logReg = LogisticRegression(penalty='l2',solver='saga',max_iter=1000,C=2,tol=1e-3,multi_class='ovr')\n",
    "logReg.fit(data_train,y_train)\n",
    "\n",
    "print(logReg.score(data_train,y_train))\n",
    "print(logReg.score(data_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para classificação multi-classe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 160 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed: 106.9min\n",
      "[Parallel(n_jobs=3)]: Done 320 out of 320 | elapsed: 181.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.4429\n",
      "acertos no teste:  0.4339\n",
      "Parâmetros: {'logistic__C': 2, 'logistic__multi_class': 'multinomial', 'logistic__penalty': 'l1', 'svd__n_components': 100, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "svd = TruncatedSVD(n_components=5, n_iter=100)\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('svd', svd), ('logistic', logReg)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b', r'\\b\\w\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'svd__n_components': [5,10,20,50,100],\n",
    "    'logistic__penalty': ['l1'],\n",
    "    'logistic__C': [0.5,1,1.5,2],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   8 out of   8 | elapsed: 114.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.44483333333333336\n",
      "acertos no teste:  0.4346\n",
      "Parâmetros: {'logistic__C': 2, 'logistic__multi_class': 'multinomial', 'logistic__penalty': 'l1', 'svd__n_components': 110, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "dictionary = pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "\n",
    "y = dictionary.target\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "svd = TruncatedSVD(n_iter=1000)\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('svd', svd), ('logistic', logReg)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'svd__n_components': [110],\n",
    "    'logistic__penalty': ['l1'],\n",
    "    'logistic__C': [2],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste para classificação binária "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=3)]: Done 128 out of 128 | elapsed: 51.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertos no treino:  0.8554333333333334\n",
      "acertos no teste:  0.8568\n",
      "Parâmetros: {'logistic__C': 2, 'logistic__multi_class': 'ovr', 'logistic__penalty': 'l1', 'svd__n_components': 50, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__token_pattern': '\\\\b\\\\w+\\\\b'}\n"
     ]
    }
   ],
   "source": [
    "dictionary = pickle.load(open('imdbCriticas.p','rb'))\n",
    "\n",
    "X = dictionary.data\n",
    "\n",
    "y = dictionary.target\n",
    "\n",
    "y = convert_indexes_into_binary(y)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=1/4,shuffle=True,random_state=0)\n",
    "\n",
    "x_train = clear_data(x_train,False)\n",
    "\n",
    "x_test = clear_data(x_test,False)\n",
    "\n",
    "x_train = stemming(x_train,3)\n",
    "\n",
    "x_test = stemming(x_test,3)\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "logReg = LogisticRegression(solver='saga',max_iter=1000)\n",
    "\n",
    "svd = TruncatedSVD(n_components=5, n_iter=100)\n",
    "\n",
    "pipe = Pipeline([('tfidf', tf_idf), ('svd', svd), ('logistic', logReg)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [3, 5],\n",
    "    'tfidf__token_pattern': [r'\\b\\w+\\b'],\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'svd__n_components': [5,10,20,50],\n",
    "    'logistic__penalty': ['l1'],\n",
    "    'logistic__C': [0.5,1,1.5,2],\n",
    "    'logistic__multi_class': ['multinomial','ovr']\n",
    "}\n",
    "\n",
    "gSearch = GridSearchCV(pipe,param_grid=param_grid,cv=2,verbose=1,n_jobs=3)\n",
    "gSearch.fit(x_train,y_train)\n",
    "\n",
    "svm = gSearch.best_estimator_\n",
    "par = gSearch.best_params_\n",
    "\n",
    "print('acertos no treino: ',svm.score(x_train,y_train))\n",
    "print('acertos no teste: ',svm.score(x_test,y_test))\n",
    "print('Parâmetros:',par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
